I have been working on a from scratch (minus [a math libaray][mathnet]) nerual network framework written in C# named [Neural.NET][my-app]. My goal has been to not only use this as a way to better understand the inner workins of nerual networks and machine learning in general, but to make it more accessible to developers. I helped my brother with a Python nerual network for one of his classes and have been interested ever since.

I have experience with [TensorFlow][tf] and used their library when they first open sources it back in November of 2015. While it is extremely powerful and fairly easy to use, it definitely requires a decent understanding of how neural networks work. I will go over my implementation in later posts, as that should be enough backstore for now.

While working through version 2, where I will add convolutional network support, I came across a method I haven't seen in most expainations of convolutional neural networks that allows you to do all the convolutions for a given layer, regardless of how many input dimenstions or kernels you have in a layer, in a single matrix matrix multiplication. While this may require a larger chunk of memory at a given time, computationally I feel this is a major improvement. Especially when I start using my network with CUDA processing, most APIs require the object to be copied to the GPU, then the computation can be done, then it needs to be copied back. If you condense all of the operations to one large operation, that should greatly reduce the amount of time CUDA operations copy and instead focus on one large operation to speed up a network.

Most people imagine a convolutional layer as having many kernels that slide across an image. The few bits of pseudo code that is shown usually involves actually figuring out what the kernel needs to be looking at, and moving it by the stride side, and repeating. They then say this is repeated for the number of kernels you have in that layer. When talking through this with my brother, he asked if there was any way to do this in bulk.

It was then that it dawned on me that a kernel convolution is idential to a vector vector multiplication. If you then can somehow calculate what pixels the kernel will convolve over, you can line those up as columns and you can do a vector matrix multiplication of a filter over an entire image. Each column of the matrix is the patch of pixels the kernel is colvolving over. You can flatten a kernel (say 5x5) into a vector (in this case length 25) by wrapping each row onto the row above it. At the end of this vector vector multiplication you have a row of pixels that is a 1D representation of the image with the filter passed over each patch.

We can still maintain spacial data, since you can construct the transformed image (mask as I refer to it later) that the filter multiplies with so the first row get processed first, then the second, and so on. You can easily put the image back into a 2D form by taking the square root (assuming square images) of the length of the resulting row and 'chopping' the image at each interval of that number. You will then have a square image that is identical to what you would have gotten by sliding the kernel over the image as most illustrations show.

In addition to this, you might easily be able to tell how you can add in all of the other filters into this operation. Instead of a vector matrix multiplication where the vector is the filter and the matrix is the 'pre convolution mask', you can stack more filters with the original filter and now you end up with a matrix matrix multiplication. Each row in the first matrix is a flattened kernel, and each column in the second matrix is the data patch that will be convolved. You will then end up with a matrix as output, where each row is a dimension of the image you can pass to another layer (be it ReLU, pooling, another convolution, or a fully connected).

I'll include some graphics in the near future.

[my-app]: https://github.com/JEMeyer/Neural.NET
[mathnet]: https://numerics.mathdotnet.com/
[tf]: https://www.tensorflow.org/